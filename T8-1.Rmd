---
title: 'Tutorial 8: Data Mining Basics'
author: "REPLACE WITH YOUR NAME"
date: "Due by 1 April 2022, 8:00 AM"
output: html_document
---

## Submission Instructions

- Select `output: html_document`.
- Include all code chunks, so include `echo=TRUE` in all chunks.
- Replace the placeholder text, "Type your answer here.", with your own.
- Submit *only* the required question for grading (Part 2: Submission). You can delete everything else for that submission. Remember to include any `library('package_name')` statements that you'll need to run your code and future reproduction. 
- Rename your R Markdown file `T[X]_[MatricNumber].rmd`, and the output will automatically be `T[X]_[MatricNumber].html`. 
- Submit your both R Markdown file (.rmd) and HTML (.html) to Luminus for tutorial assignments (upload to Luminus under the correct Submission Folder). We shall do the same for practical exam.
- **It is important to be able to code and produce your Rmarkdown output file *independently*.** You are responsible for de-bugging and programming in the practical exam.

```{r load-libraries, echo=TRUE, warning = FALSE, message = FALSE}
# load required packages
library(dplyr)
library(tidyr)
library(car) # for linearHypothesis()
library(ggplot2) # optional. we expect you to know base graphics, but allow ggplot if you find it easier
library(psych) # for pairs.panels()
library(factoextra) # for fviz_cluster()
library(wooldridge)
```



## Part One: Lab Session Completion and Discussion

### Question 1 


- Dataset required: `whiskies.csv`

This will be an exploratory question using k-means clustering to examine a dataset of Whiskey Taste Indicators. The dataset can be obtained from https://outreach.mathstat.strath.ac.uk/outreach/nessie/nessie_whisky.html. 

It consists of 86 (Single-Malt) Whiskies that are rated from 0-4 on 12 different taste categories: `Body`, `Sweetness`, `Smoky`, `Medicinal`, `Tobacco`, `Honey`, `Spicy`, `Winey`, `Nutty`, `Malty`, `Fruity`, `Floral`.

Here's what the dataset looks like:

```{r q1-read-in-dataset, echo=TRUE, fig.width=10}
wh = read.csv('whiskies.csv', header=T)

# Selecting out the independent variables "X".
whX <- wh %>% select(c("Body", "Sweetness", "Smoky", "Medicinal", "Tobacco", "Honey", "Spicy", "Winey", "Nutty", "Malty", "Fruity", "Floral"))

# using pairs.panel() to look at the data
pairs.panels(whX, lm=T)
```

The main purpose of this question is to try clustering a real dataset, and try to interpret the clusters via looking at the cluster centers (in the dimensions of the independent variables), and generating "profiles" for each cluster.


Q1a) 

Let's try clustering the different whiskies based on their taste profile. First, let's use the Elbow method to pick the best number of clusters.

Using the code discussed in lecture, calculate the Within-Cluster Sum of Squares from k=2 to k=20 clusters using `whX`, and plot the Within-Cluster Sum of Squares against number of clusters.


*Note*, normally if the variables are on very different scales, we should standardize the variables (to have mean 0 and sd 1). But in this case, all the variables are on the same scale; they're all 0-4. So even if they may have different means and SDs, it's ok to NOT scale the variables. Just run kmeans on `whX`.

<p style="color:red">**Type your answer here.**</p>


```{r, q1a-elbow-plot, echo=TRUE}
# type your code here
set.seed(1)
wss1 = rep(NA, 20)
for (k in c(2:20)){
  wss1[k] = kmeans(whX, k, nstart=10)$tot.withinss
}

plot(wss1,
     type="b",
     xlab="number of clusters, k",
     ylab="within cluster sum of squares distance")

```



Q1b)

From the plot, it does not seem like there is a clear Elbow. The Within-Cluster Sum of Squares seem to keep decreasing, and there doesn't seem to be a clear stopping point. This may happen in real datasets (and this is a real dataset), so we may have to use our own judgment to decide on the number of clusters.

Ok, let's say our local business partner applies his expert intuition, and tells us that `k=3` seems a good starting point.

Because the output of k-means depends upon the random initialization, we need to set the seed of the random number generator, so that all of us (students+TAs+instructors) can get the same results.

<p style="color:green">**Run the following code the line before your k-means code.**</p>

```
set.seed(1)
... = kmeans(...)
```


Then use the `fviz_cluster()` function from the `factoextra` package to plot the results of this clustering.

What do you notice from the graph? Discuss this with your TA and fellow students. (Recall that the graph dimensions will be along the top two principal components.)


<p style="color:red">
Again, no answers needed here, except the graph. Discussion points here are that there seems to be three clearly separated clusters: one (in the graph above, Cluster 1) that's much higher on PC1 than the rest (i.e., on the right of the graph). The other two (Clusters 2 and 3) are on the left side of the graph, buth they are separeted by PC2, such that Cluster 2 is higher on PC2 and Cluster 3 is lower on PC2.
</p>


```{r q1b-viz, echo=T}
# type your code here
set.seed(1)
km_whX <- kmeans(whX, centers = 3)

fviz_cluster(km_whX,
             data=whX,
             main = "Three clusters on the plane of first two PCs of ’whX’.")

```



Q1c)

Ok, let's use `<kmeans_object_name> $center` (where `<kmeans_object_name>` is the name of the kmeans model you fit above) to extract the centers of the 3 clusters.

Try to interpret the clusters. I'll provide you with one observation, please generate at least four more.

- I notice that Cluster 1 has the highest Body compared to Clusters 2 and 3.

After generating several of these observations, try to summarize your observations into a Taste Profile for each Cluster. FOR EXAMPLE, is Cluster 1 high in Smoky, Tobacco-y, Spicyness?

If your client really likes very rich, Smoky, Medicinal Whiskies, which cluster would you recommend to him? (e.g., if this were a real client, you could go back and look at the Distilleries in `wh` and generate a list of those in the same cluster.)


<p style="color:red">
Cluster 1 will be fuller bodied, less sweet, more smokey, more medical, more tobaccoy, less honey, less fruity and less floral than the rest. This is probably what PC1 is picking up on (what we saw in Q1b). Clusters 1 and 2 are relatively more similar to each other, but compared to Cluster 2, Cluster 1 has: less body, less honey, less "whiney" and nutty tastes.
</p>

```{r q1c-centers, echo=TRUE}
# type your code here
km_whX$center
```


## Part Two: Assignment Submission 

### Question 2 (total 15 points)


- Dataset required: `attend` (wooldridge)

In this question, we will be doing a model selection, principal component analysis, building a simple logit classifier, and finally assessing the output of that classifier for class performance.

The dataset for this question is available at: https://rdrr.io/cran/wooldridge/man/attend.html. a public available dataset about class attendance and final/GPA performance. Again, you need to install and load package `wooldridge` to conveniently load the data into your R workplace. 

```{r q2-dataloading, echo=TRUE}
data('attend')
```

Here are the variables in the dataset:

- `attend`: classes attended out of 32.
- `termGPA`: GPA for current term.
- `priGPA`: cumulative GPA prior to current term.
- `ACT`: ACT score.
- `final`: final test score.
- `atndrte`: percentage of class attendance, i.e. `attend` divided by `32`.
- `hwrte`: percentage of homework turned in.
- `frosh`: freshme if = 1.
- `soph`: sophomore if = 1.
- `missed`: number of classess missed, i.e. `attend` + `missed` = 32.
- `stndfnl`: standardized final test score, i.e. (`final`-mean)/sd.

In the code below I make a new variable `pass` (pass the exam) which is one if student's final performance belongs to upper 40% of class comparable to his/her peers (curved) based on the standardized final test score, i.e. `stndfnl` is greater than 60th-quantile of `stndfnl`, This would be a binary dependent variable we shall use.

In this question, we will be interested in using the independent variables (student's class attendance) to classify the sample into whether student will pass or not. 

Here are what the independent variables look like (using the `pairs.panels()` function from the `psych` package)

```{r q2-read-in-dataset, echo=TRUE, fig.width=10}
# create a binary variable 'pass',
attend$pass = ifelse(attend$stndfnl > quantile(attend$stndfnl, 0.6), 1, 0)
# removing NA's in the data, just to avoid some programming issues later. WARNING: don't simply do this in your future projects.
attend = attend[complete.cases(attend),]
# Selecting out the independent variables "X".
attendX = attend %>% select(c("attend", "termGPA", "priGPA", "ACT", "hwrte"))
psych::pairs.panels(attendX, lm=TRUE)
```
(Q2a)
Let's first start with our "kitchen sink" regression model `stndfnl ~ attend + termGPA + priGPA + ACT + hwrte + frosh + soph` with entire data set `attend`. (1) using `linearHypothesis()` to jointly test if `termGPA = priGPA = ACT = 0`, i.e. current and previous GPA/test scores do not affect the final test score, and draw your conclusion for the test; (2) run a automated backward model selection using `step()` function and interpret the coefficient of `attend`. (4 points)

<p style="color:red">**Type your answer here.**</p>

```{r, q2a1-jointtest, echo=TRUE}
# type your code here
```

Q2b)
From the correlation matrix at the very beginning, we can see that many of the independent variables are highly correlated with each other. Let's try to summarize the data using principal component analysis (PCA).

- Use the `prcomp` function to conduct a PCA. (We'll have to feed `attendX`, not `attend`, into `prcomp`)
- What is the cumulative proportion of variance explained by the first four PCs?
- Extract the first four PCs and pass them to `attend`. We'll be using these as predictors.

(3 points)

By using `<pca_object>$rotation[,1:3]`, you can see the loadings on the first 3 PCs.
We will not be attempting to interpret the PCs in this question because it's generally hard to come up with a meaningful interpretation over principal components which are just indices. 
The only thing to be pointed out, is that PC1 is **negatively** correlated with **every** single variable (to verify if you did it right). Now, Principal Components are just vectors in some high-dimensional space, and so actually it doesn't make sense to tell whether it's a vector pointing right or pointing left, it's just how it is pointing relative to all the other variables. So we can guess, not being trained as an education professional, is that PC1 roughly captures the characteristics of a "lazy" student. (We can thus immediately make a prediction as to the sign of the coefficient if we used PC1 to predict final test score, even before we run Q2c below... Try to guess!)

But in general I would recommend that if you do end up using PCA for your future works, to at least attempt to interpret the PCs, in the spirit of understanding more about the data.

<p style="color:red">**Type your answer here.**</p>

```{r, q2b-pca, echo=TRUE}
# type your code here
```


Q2c)

Run a logistic regression of `pass` on the top four principal components. Which coefficients are statistically significant? 

Using a model with all four PCs, use `predict(<glm_object>, type='response')` to ask the model to predict the probability of pass. Let's make our rule to define the predicted value of `pass`: being equal to one (predicted "pass") if the predicted probability is >= 0.60; and zero otherwise (predicted "fail"). Pass the binary predictions to a variable named `pred_pass` in `attend`. How many "Yes" and "No" predictions did the model make? (4 points)



<p style="color:red">**Type your answer here.**</p>

```{r, q2c-logitclassifier, echo=TRUE}
# type your code here
```



Q2d) Finally, let's manually construct a classification matrix using `table()` function in base R rather than `caret::confusionMatrix()`.

Use `table(x1, x2)` with both your model's "Pass/Fail" predictions and the actual observed `pass` values. I recommend using the same convention in the lecture slides, where we have actual values on the columns and model prediction on the rows. *We say "pass" is defined as positive event.* (4 points)

- How many True Positives are there?
- How many True Negatives are there?
- How many False Positives are there?
- How many False Negatives are there?

- What is the model's overall classification accuracy?
- What is the model's sensitivity?
- What is the model's precision?
- What is the model's specificity?

<p style="color:red">**Type your answer here.**</p>


```{r q2d-ClassificationMatrix, echo=TRUE}
# type your code here
```

